---
title: "Chapter 7: Data Cleaning and Preparation"
output: html_notebook
---

## Handling Missing Data

In pandas, all of the descriptive statistics exclude missing data by default. For numeric data, pandas uses the floating-point value `NaN` to represent missing data. This value can easily be detected:

```{python}
string_data = pd.Series(["aardvark", "artichoke", np.nan, "avocado"])
string_data

string_data.isnull()
```

The Python value `None` is also treated as NA.

### Filtering Out Missing Data

There are a few ways to filter out missing data. You can always do it by hand using `pandas.isnull`, but the `dropna` can be helpful. One a Series, it returns the Series with only the non-null data and index values.

```{python}
from numpy import nan as NA

data = pd.Series([1, NA, 3.5, NA, 7])

data.dropna()
```

This is equivalent to:

```{python}
data[data.notnull()]
```

With DataFrame objects you may want to drop rows or columns that are all NA or only those containing any NAs. `dropna` by default drops any row containing a missing value:

```{python}
data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA],
                    [NA, NA, NA], [NA, 6.5, 3.]])

cleaned = data.dropna()

data

cleaned
```

Passing `how = "all"` will only drop rows that are all NA:

```{python}
data.dropna(how = "all")
```

Or you can drop columns that are all NA with `axis = 1`:

```{python}
data[4] = NA

data

data.dropna(axis = 1, how = "all")
```

Suppose you want to keep only rows containing a certain number of observations. You can do this with the `thresh` argument:

```{python}
df = pd.DataFrame(np.random.randn(7, 3))

df.iloc[:4, 1] = NA

df.iloc[:2, 2] = NA

df

df.dropna()

df.dropna(thresh = 2)
```

### Filling in Missing Data

Instead of filtering out missing data, you may want to fill it in. The `fillna` method is the main way to do this.

```{python}
df.fillna(0)
```

Calling `fillna` with a dict, you can use a different fill value for each column:

```{python}
df.fillna({1: 0.5, 2: 0})
```

`fillna` returns a new object, but you can modify the existing object in-place:

```{python}
_ = df.fillna(0, inplace = True)

df
```

The same interpolation methods available for reindexing can be used with `fillna`:

```{python}
df = pd.DataFrame(np.random.randn(6, 3))

df.iloc[2:, 1] = NA

df.iloc[4:, 2] = NA

df

df.fillna(method = "ffill")

df.ffill()

df.fillna(method = "ffill", limit = 2)
```

With `fillna` you can fill in other values. For example, you can pass the mean or median value of a Series:

```{python}
data = pd.DataFrame([1., NA, 3.5, NA, 7])

data.fillna(data.mean())
```

## Data Transformation

### Removing Duplicates

Here is an example of duplicate rows:

```{python}
data = pd.DataFrame({"k1": ["one", "two"] * 3 + ["two"],
                     "k2": [1, 1, 2, 3, 3, 4, 4]})

data
```

The DataFrame method `duplicated` returns a Series identifying if each row is a duplicate or not:

```{python}
data.duplicated()
```

`drop_duplicates` returns a DataFrame where the `duplicated` array is `False`:

```{python}
data.drop_duplicates()
```

These methods consider all columns by default. We could instead specify a subset of them to detect duplicates. Suppose we had an additional column of values and wanted to filter duplicates only based on the `k1` columns:

```{python}
data["v1"] = range(7)

data.drop_duplicates(["k1"])
```

By default, the first observed duplicate is kept, but you can pass `keep = "last"` to keep the last duplicate:

```{python}
data.drop_duplicates(["k1", "k2"], keep = "last")
```

### Transforming Data Using a Function or Mapping

You might want to do some transformation based on the values in an array, Series, of column in a DataFrame. Consider this data:

```{python}
data = pd.DataFrame({"food": ["bacon", "pulled pork", "bacon",
                              "Pastrami", "corned beef", "Bacon",
                              "pastrami", "honey ham", "nova lox"],
                     "ounces": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})

data
```

If we wanted to add a column indicating the type of animal that each food came from we could write a mapping function:

```{python}
meat_to_animal = {
  "bacon": "pig",
  "pulled pork": "pig",
  "pastrami": "cow",
  "corned beef": "cow",
  "honey ham": "pig",
  "nova lox": "salmon"
}
```

Then use the `map` method .

```{python}
lowercased = data["food"].str.lower()
lowercased

data["animal"] = lowercased.map(meat_to_animal)
data
```

We could also have passed a function that does all the work:

```{python}
data["food"].map(lambda x: meat_to_animal[x.lower()])
```

### Replacing Values

`replace` can be used to replace values.

```{python}
data = pd.Series([1., -999., 2., -999., -1000., 3.])
data
```

The `-999` here is a missing value. We can replace these with NA using the `replace` method.

```{python}
data.replace(-999, np.nan)
data

# data.replace(-999, np.nan, inplace = True)
# data
```

You can also replace multiple values at once:

```{python}
data.replace([-999, 1000], np.nan)
```

To use a different replacement for each value, pass a list of substitutes:

```{python}
data.replace([-999, -1000], [np.nan, 0])
```

### Renaming Axis Indexes

Axis labels can be transformed by a function or mapping of some form to produce new, differently labeled objects.

```{python}
data = pd.DataFrame(
  np.arange(12).reshape((3, 4)),
  index = ["Ohio", "Colorado", "New York"],
  columns = ["one", "two", "three", "four"]
)
```

The axis indexes have a `map` method

```{python}
transform = lambda x: x[:4].upper()

data.index.map(transform)
```

You can assign to `index`, modifying the DataFrame in-place:

```{python}
data.index = data.index.map(transform)
data
```

If you want to create a transformed version of a dataset without modifying the original, a useful, a useful method if `rename`:

```{python}
data.rename(index = str.title, columns = str.upper)
```

`rename` can also be used to to provide new values for a subset of axis labels:

```{python}
data.rename(index = {"OHIO": "INDIANA"}, columns = {"three": "peekaboo"})
data
```

You can also modify the DataFrame inplace to save you the step of copying it:

```{python}
data.rename(index = {"OHIO": "INDIANA"}, inplace = True)
data
```

### Discretization and Binning

Suppose you have a group of people in a study, and you want to group them into discrete age buckets:

```{python}
ages = [20, 22,25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
```

Let's divide these into bins of 10 to 25, 26 to 35, 36 to 60, and finally 61 and older with the `cut` function:

```{python}
bins = [18, 25, 35, 60, 100]

cats = pd.cut(ages, bins)
cats
```

This returns a `Categorical` object.

```{python}
cats.codes

cats.categories

pd.value_counts(cats)
```

You can also pass bin names using the `labels` option:

```{python}
group_names = ["Youth", "Young Adult", "MiddleAged", "Senior"]

cats = pd.cut(ages, bins, labels = group_names)

pd.value_counts(cats)
```

### Detecting and Filtering Outliers

Let's start with some normally distributed data:

```{python}
data = pd.DataFrame(np.random.randn(1000, 4))
data

data.describe()
```

Suppose we want to find values in one of the columns exceeding 3 in absolute value:

```{python}
col = data[2]
col

col[np.abs(col) > 3]
```

To select all rows having a value exceeding 3 or -3, you can use the `any` method on a DataFrame:

```{python}
data[(np.abs(data) > 3).any(1)]
```

Values can be set based on these criteria. Here is code to cap values outside the interval -3 to 3:

```{python}
data[np.abs(data) > 3] = np.sign(data) * 3

data.describe()
```

The statement `np.sign(data)` produces 1 and -1 values based on whether the values in `data` are positive or negative:

```{python}
np.sign(data).head()
```

### Permutation and Random Sampling

Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do using the `numpy.random.permutation` function. Calling `permutation` with the length of the axis you want to permute produces an array of integers indicating the new ordering:

```{python}
df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))

sampler = np.random.permutation(5)
sampler
```

That array can then be used in `iloc`-based indexing or the equivalent `take` function:

```{python}
df

df.take(sampler)
```

To select a random subset without replacement, you use the `sample` method:

```{python}
df.sample(n = 3)
```

To generate a sample with replacement, pass `replace = True` to `sample`:

```{python}
choices = pd.Series([5, 7, -1, 6, 4])

draws = choices.sample(n = 10, replace = True)
draws
```

### Computing Indicator/Dummy Variables

pandas has a `get_dummies` function for generating dummy variables.

```{python}
df = pd.DataFrame(
  {"key": ["b", "b", "a", "c", "a", "b"],
  "data1": range(6)}
)

df

pd.get_dummies(df["key"])
```

You can also add a prefix to the columns in the indicator DataFrame:

```{python}
dummies = pd.get_dummies(df["key"], prefix = "key")
dummies

df_with_dummy = df[["data1"]].join(dummies)
df_with_dummy
```

What if a row in a DataFrame belongs to multiple categories?

```{python}
mnames = ["movie_id", "title", "genres"]

movies = pd.read_table("datasets/movielens/movies.dat", sep = "::",
                       header = None, names = mnames)

movies[:10]
```

Adding an indicator for each genre requires more work. First, we extract the list of unique genres in the dataset:

```{python}
all_genres = []

for x in movies.genres:
    all_genres.extend(x.split("|"))
    

genres = pd.unique(all_genres)
genres
```

One way to construct the indicator DataFrame is to start with a DataFrame of all zeros:

```{python}
zero_matrix = np.zeros((len(movies), len(genres)))

dummies = pd.DataFrame(zero_matrix, columns = genres)
dummies
```

No, iterate through each movie and set entries in each row of `dummies` to 1. To do this, we use the `dummies.columns` to compute the column indices for each genre:

```{python}
gen = movies.genres[0]
gen

gen.split("|")

dummies.columns.get_indexer(gen.split("|"))
```

Then, we can use `.iloc` to set values based on these indices:

```{python}
for i, gen in enumerate(movies.genres):
    indices = dummies.columns.get_indexer(gen.split("|"))
    dummies.iloc[i, indices] = 1


```

Then you can combine this with movies:

```{python}
movies_windic = movies.join(dummies.add_prefix("genre_"))

movies_windic.iloc[0]

movies_windic.head()
```

Here is how we accomplish this in R:

```{r}
library(tidyverse)
library(reticulate)

head(py$movies)

py$movies %>% 
  separate(genres, into = c(str_c("genre", c(1:10), sep = "_")), sep = "\\|") %>%
  pivot_longer(cols = genre_1:genre_10, 
               names_to = "genre_num", 
               values_to = "genre") %>% 
  select(-genre_num) %>% 
  drop_na(genre) %>% 
  fastDummies::dummy_cols("genre") %>% 
  select(-genre) %>% 
  group_by(movie_id) %>% 
  summarise(title = first(title),
            across(genre_Action:genre_Western, sum))
```

It can also be useful to combine `get_dummies` with functions like `cut`:

```{python}
np.random.seed(12345)

values = np.random.rand(10)
values

bins = [0, 0.2, 0.4, 0.6, 0.8, 1]

pd.get_dummies(pd.cut(values, bins))
```

## String Manipulation

### String Object Methods

Python's built-in string methods can be sufficient. For example, a comma-separated string can be broken into pieces with split:

```{python}
val = "a,b, guido"

val.split(",")
```

`split` is often combined with `strip` to trim whitespace:

```{python}
pieces = [x.strip() for x in val.split(",")]
pieces
```

You concatenate strings with a two-colon delimiter and addition:

```{python}
first, second, third = pieces

first + "::" + second + "::" + third
```

A faster way is to pass a list to the `join` method on the string `"::"`:

```{python}
"::".join(pieces)
```

Python's `in` keyword is the best way to detect a substring, though `index` and `find` can also be used:

```{python}
"guido" in val

val.index(",")

val.find(":")
```

`count` returns the number of occurrences of a particular substring:

```{python}
val.count(",")
```

`replace` will substittue occurrences of one pattern for another. It is commonly used to delete patterns, too, by passing an empty string:

```{python}
val.replace(",", "::")

val.replace(",", "")
```

### Regular Expressions

The built-in `re` module is responsible for applying regular expressions to strings. It falls into three categories: pattern matching, substitution, and splitting. Suppose we wanted to split a string with a variable number of whitespace characters. The regex describing one or more whitespace characters is `\s+`:

```{python}
import re

text = "foo     bar\t baz \tquz"

re.split("\s+", text)
```

`search` returns only the first regex match and `match` only matches at the beginning of the string.

```{python}
text = """Dave dave@google.com
Steve steve@gmail.com
Rob rob@gmail.com
Ryan ryan@yahoo.com
"""

pattern = r"[A-Z0-9._%-]+@[A-Z0-9.-]+\.[A-Z]{2,4}"

regex = re.compile(pattern, flags = re.IGNORECASE)
```

Using `findall` on the text produces a list of the email addresses:

```{python}
regex.findall(text)
```

`search` returns a special match object for the first email address in the text.

```{python}
m = regex.search(text)
m

text[m.start():m.end()]
```

`regex.match` returns `None` because it only returns patterns that occur at the start of the string:

```{python}
print  (regex.match(text))
```

### Vectorized String Functions in pandas

Sometimes strings can have missing data:

```{python}
data = {
  "Dave": "dave@google.com", 
  "Steve": "steve@gmail.com",
  "Rob": "rob@gmail.com",
  "Wes": np.nan
}

data = pd.Series(data)
data
```

Regular expressions fail with missing values so we use array-oriented methods that skip NAs. These are access through a `str` attribute:

```{python}
data.str.contains("gmail")
```
